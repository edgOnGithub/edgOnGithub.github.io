---
output: 
  html_document:
    code_folding: show
    highlight: kate
    theme: paper
editor_options:
  chunk_output_type: console
---

Visualising London Underground data.
```{r, include = FALSE}
library(readr)
library(dplyr)
library(knitr)

```


## Cleaning Data

First we select only London Underground data from TFL's transport CSV.

```{r initial_chunk, cache = TRUE}
library(readr)
library(dplyr)
library(knitr)
oyster <- read_csv('assets/Original Data/Nov09JnyExport.csv') 
oyster_underground <- oyster %>% 
  filter(SubSystem == "LUL")
rm(oyster)
gc()

oyster_underground <- oyster_underground %>% 
  filter(StartStn != "Unstarted") %>% 
  filter(EndStation != "Unfinished") %>% 
  filter(!is.na(EntTimeHHMM)) %>% 
  filter(!is.na(EXTimeHHMM)) %>% 
  select(-SubSystem)

oyster_underground %>% 
  head() %>% 
  kable()






```
Next we encode `daytype` as a factor with levels running from Monday to Sunday.
```{r initial_cleaning}
library(forcats)
library(lubridate)
oyster_underground$daytype <- factor(oyster_underground$daytype, c("Mon",
                                                                   "Tue",
                                                                   "Wed",
                                                                   "Thu",
                                                                   "Fri",
                                                                   "Sat",
                                                                   "Sun",
                                                                   "Weekend",
                                                                   "Weekday"))

oyster_underground_lubridate <- oyster_underground %>% 
  mutate(entry_time = EntTimeHHMM %>% 
           as.POSIXct() %>% 
           ymd_hms(),
         exit_time = EXTimeHHMM %>% 
           as.POSIXct() %>% 
           ymd_hms(),
         journey_time = interval(entry_time, exit_time) %>% 
           time_length(unit = "minute")) %>% 
  filter(journey_time > 0)
```



## Initial Visualisations

Using Plotly we can create histograms  
```{r initial_histograms, fig.show='hold', fig.align="center"}
library(ggplot2)
library(tidyr)
p <- ggplot(oyster_underground_lubridate,
            aes(x = EntTimeHHMM)) +
  geom_histogram(fill = "firebrick2", binwidth = 600, alpha = 0.2, color = "white") +
  theme_minimal() +
  xlab("Tap In Time") +
  ggtitle("Tap In Time for London Tube Stations") 
p

q <- ggplot(oyster_underground_lubridate,
            aes(x = EXTimeHHMM)) +
  geom_histogram(fill = "springgreen2", binwidth = 600, colour = "white") +
  theme_minimal() +
  ggtitle("Tap Out Time for London Tube Stations") +
  xlab("Tap Out Time")
q



```


Now we combine the two together in a density plot:

```{r overlaid_densities, fig.align="center"}

pq <- oyster_underground_lubridate %>% 
               select("Tap In" = EntTimeHHMM,"Tap Out" = EXTimeHHMM) %>%
               gather(type, time, "Tap In", "Tap Out") %>% 
  ggplot(aes(x = time, fill = type)) +
  geom_density(alpha = 0.2) +
  scale_fill_manual(values = c("firebrick2", "springgreen2"), "Type") +
  xlab("Time") +
  ggtitle("Tap In And Tap Out Time", subtitle = "London Tube Stations") +
  theme_minimal()
pq


```



```{r journey_times, fig.align="center"}
library(RColorBrewer)
library(plotly)


oyster_j_time <- oyster_underground_lubridate %>% 
  group_by(entry_time, daytype, downo) %>% 
  summarise(average_journey = mean(journey_time))

oyster_j_time_5 <- oyster_j_time %>% 
  mutate(five_min = round_date(entry_time, unit = "5 min")) %>% 
  group_by(five_min, daytype, downo) %>% 
  summarise(average_journey_5 = mean(average_journey))
  
# 
# oyster_j_time_5 %>% 
#   plot_ly(x = ~five_min,
#           y = ~average_journey_5,
#           color = ~daytype,
#           mode = "markers",
#           type = "scatter",
#           marker = list(color = brewer.pal(9, name = "Blues")))
# 
# oyster_j_time_5 %>% 
#   plot_ly(x = ~five_min,
#           y = ~average_journey_5,
#           colors = "Blues",
#           color = ~daytype,
#           mode = "markers",
#           type = "scatter")

p <- oyster_j_time_5 %>% 
  ggplot(aes(x = five_min,
             y = average_journey_5,
             colour = daytype)) +
  geom_point() +
  scale_color_brewer(palette = "OrRd", direction = -1, "Day") +
  theme_minimal() +
  xlab("Time") +
  ggtitle("Mean Journey Length For a Given Start Time") +
  ylim(0, 75) +
  labs(subtitle = "Lighter colours indicate days later in the week")
p

q <- oyster_j_time_5 %>% 
  ggplot(aes(x = five_min,
             y = average_journey_5,
             colour = daytype)) +
  geom_smooth(se = FALSE, size = 2) +
  scale_color_brewer(palette = "OrRd", direction = -1, "Day") +
  theme_minimal()
q

```



Creating station data
```{r creating_station_data}

stations_data <- oyster_underground_lubridate %>% 
  group_by(StartStn, EndStation) %>% 
  summarise(n = n(),
            ave_time = mean(journey_time, na.rm = TRUE),
            FFare_mean = mean(FFare, na.rm = TRUE)) %>% 
  arrange(-n)



```



```{r initial_graphs, fig.height=10, fig.width=10}
library(tidygraph)
library(ggraph)
stations_filtered <- stations_data %>%
  ungroup() %>% 
  top_n(wt = n, 15) %>% 
  select(StartStn, EndStation) %>% 
  gather(type, station) %>% 
  select(station)




station_data_filtered <- stations_data %>% 
  mutate(x = rnorm(n())) %>% 
  arrange(x) %>% 
  filter((StartStn %in% stations_filtered$station) | (EndStation %in% stations_filtered$station)) %>% 
  filter(n > 250)



oyster_graph <- as_tbl_graph(station_data_filtered, directed  = TRUE)
ggraph(oyster_graph, layout = "linear", circular = TRUE) + 
    geom_edge_arc(aes(width = n, color = n, alpha = n,
                      
                      start_cap = label_rect(node1.name),
                       end_cap = label_rect(node2.name))) +
    guides(edge_width = 'none', edge_color = "none") +
    geom_node_text(aes(label = name), size = 3, repel = FALSE, color = "white") +
  ggtitle("Most Common Tube Journeys") +
  scale_edge_colour_distiller(palette = "Reds", direction = 1) +
  theme_graph(background = 'grey20', text_colour = 'white') +
  theme(legend.position = "none")
  
```

```{r fan_graph, fig.height=10, fig.width=10}

# ggraph(oyster_graph) + 
#     geom_edge_fan(aes(width = n, color = n, alpha = n),
#                   arrow = arrow(length = unit(2, 'mm')),
#                   end_cap = circle(2, 'mm')) + 
#     guides(edge_width = 'none',
#            edge_colour = "none",
#            alpha = "none") +
#     geom_node_text(aes(label = name), size = 5, repel = TRUE, colour = "white") +
#     scale_edge_colour_distiller(palette = "Reds", direction = 1) +
#     theme_graph(background = "grey20", text_colour = "white") +
#   theme(legend.position = "none")
  


ggraph(oyster_graph) + 
    geom_edge_fan(aes(width = n, color = n, alpha = n,
                      start_cap = label_rect(node1.name),
                       end_cap = label_rect(node2.name)),
                  arrow = arrow(length = unit(2, 'mm'))) + 
    guides(edge_width = 'none',
           edge_colour = "none",
           alpha = "none") +
    geom_node_text(aes(label = name), size = 5, repel = FALSE, colour = "white") +
    scale_edge_colour_distiller(palette = "Reds", direction = 1) +
    theme_graph(background = "grey20", text_colour = "white") +
  theme(legend.position = "none")
  
```

```{r facetted_graph, fig.height=10, fig.width=10, eval = FALSE, echo = FALSE}


ed <- oyster_graph %>% 
         activate(edges) %>% 
         mutate(mean_journey_length = mean(ave_time),
                short_journey = ifelse((ave_time < mean_journey_length),
                                       1, 0))
ggraph(ed) + 
    geom_edge_fan(aes(width = n, color = n, alpha = n,
                      start_cap = label_rect(node1.name),
                       end_cap = label_rect(node2.name)),
                  arrow = arrow(length = unit(2, 'mm'))) + 
    guides(edge_width = 'none',
           edge_colour = "none",
           alpha = "none") +
    geom_node_text(aes(label = name), size = 5, repel = TRUE, colour = "white") +
    scale_edge_colour_distiller(palette = "Reds", direction = 1) +
    theme_graph(background = "grey20", text_colour = "white") +
  theme(legend.position = "none") +
  facet_edges(~short_journey)



```

```{r last_graph, eval = FALSE}

ggraph(oyster_graph %>% 
         activate(edges) %>% 
         mutate(mean_journey_length = mean(ave_time),
                short_journey = ifelse((ave_time < mean_journey_length),
                                       1, 0)), layout = "grid") + 
    geom_edge_fan(aes(width = n, color = n),
                  spread = 5,
                  arrow = arrow(length = unit(2, 'mm')), 
                  alpha = 0.5,
                   end_cap = circle(2, 'mm')) + 
    guides(edge_width = 'none',
           edge_colour = "none") +
    geom_node_text(aes(label = name), size = 3, repel = FALSE) +
    scale_edge_colour_distiller(palette = "Reds", direction = 1) +
    theme_graph()

## facet wrap short and long journeys

```


```{r, eval = FALSE}
switched_df <- stations_filtered %>% 
  unite(col = station_pair, EndStation, StartStn, remove = FALSE)
switched_df

station_usage <- stations_filtered %>% 
  unite(col = station_pair, StartStn, EndStation, remove = FALSE) %>% 
  ungroup() %>% 
  select(station_pair, n, StartStn, EndStation) %>% 
  inner_join(y = switched_df %>% 
               ungroup() %>% 
               select(station_pair, n), by = 'station_pair') %>% 
  mutate(summed_journeys = n.x + n.y)

station_usage

stations_filtered_journeys <- stations_filtered %>% 
  left_join(y = station_usage %>% 
              select(StartStn, EndStation,summed_journeys), by = c("StartStn", "EndStation")) 

oyster_journeys <- stations_filtered_journeys %>% 
  as_tbl_graph(directed = FALSE)


ggraph(oyster_journeys, layout = "fr") + 
    geom_edge_fan(aes(width = summed_journeys, color = n), alpha = 0.2) + 
    guides(edge_width = 'none') +
    geom_node_text(aes(label = name), size = 3, repel = FALSE) +
    scale_color_brewer(palette = "Set1") +
    theme_graph()
```



```{r time_of_day_hist, fig.align="center"}
library(magrittr)
library(scales)


wknd <- oyster_underground_lubridate %>% 
  mutate(daytype = ifelse((daytype == "Sat") | (daytype == "Sun"), "Weekend", "Weekday"))
wknd$daytype <- factor(wknd$daytype,
                 c("Mon",
                   "Tue",
                   "Wed",
                   "Thu",
                   "Fri",
                   "Sat",
                   "Sun",
                   "Weekend",
                   "Weekday"))
  
duplicated_data <- bind_rows(oyster_underground_lubridate, wknd)

duplicated_data %>% 
  filter(StartStn == "Waterloo JLE") %>% 
  ggplot(aes(x = EntTimeHHMM %>% as.POSIXct(), fill = daytype)) +
  geom_histogram(colour = "black", bins = 24) +
  facet_wrap(~daytype, scale = "free_y") +
  theme_minimal() +
  guides(fill = "none") +
  scale_fill_brewer(palette = "Set3", direction = -1, "Day") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_x_time(labels = date_format("%H:%M")) +
  xlab("Hour of Day") +
  labs(title = "Footfall Per Hour",
       subtitle = "Waterloo Tube Station",
       caption = "Note: Rescaled y-axes")

```




```{r}
duplicated_data %>% 
  ggplot(aes(x = EntTimeHHMM %>% as.POSIXct(), fill = daytype)) +
  geom_histogram(colour = "black", bins = 24) +
  facet_wrap(~daytype, scale = "free_y") +
  theme_minimal() +
  guides(fill = "none") +
  scale_fill_brewer(palette = "Set3", direction = -1, "Day") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_x_time(labels = date_format("%H:%M")) +
  xlab("Hour of Day") +
  labs(title = "Footfall Per Hour",
       subtitle = "Waterloo JLE",
       caption = "Note: Rescaled y-axes")
```



## TODO: knit together days somehow and use rolling samples to model the problem. 


```{r}
oyster_knit <- oyster_underground %>%
  filter(!(is.na(EntTimeHHMM) & is.na(EXTimeHHMM))) %>% 
  mutate(downo = sub("^", "0", downo)) %>% 
  unite(col = entry_datetime, sep = "-11-09 ", downo, EntTimeHHMM, remove = FALSE)
oyster_knit <- oyster_knit %>% 
  unite(col = exit_datetime, sep = "-11-09 ", downo, EXTimeHHMM, remove = FALSE)
  
oyster_dates <- oyster_knit %>% 
  mutate(entry_datetime = dmy_hms(entry_datetime),
         exit_datetime = dmy_hms(exit_datetime))



ggplot(oyster_dates,
       aes(x = entry_datetime)) +
  geom_histogram(binwidth = 600)
  
```

## Data Manipulation

```{r}
library(tibbletime)
pooled_data <- oyster_dates %>% 
  group_by(entry_datetime) %>% 
  summarise(y = n(),
            daytype = first(daytype),
            fare = mean(DFare, na.rm = TRUE)) %>% 
  rename(date = entry_datetime) %>% 
  as_tbl_time(index = date)
pooled_data


earliest_time <- pooled_data$date %>% min()
latest_time <- pooled_data$date %>% max()



empty_ts <- create_series(earliest_time ~ latest_time, "1 minutes")
pooled_time_data <- left_join(empty_ts, pooled_data, by = "date") %>% 
  mutate(y = ifelse(is.na(y), 0, y),
         fare = ifelse(is.na(fare), 0, fare),
         daytype = wday(date, label = TRUE))
pooled_time_data

pooled_time_data %>% 
  ggplot(aes(x = date,
             y = y,
             colour = y)) + 
  geom_point()




pooled_weekday <- pooled_time_data %>% 
  filter(!((daytype == "Sat") | (daytype == "Sun"))) %>% 
  select(index = date, y) %>% 
  as_tbl_time(index = index)


  ggplot(pooled_weekday,
         aes(x = index,
             y = y)) +
  geom_point()

```


## Very similar to this blog post
https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/

```{r}
library(tidyquant)
library(timetk)
library(glue)
library(cowplot)

library(rsample)
library(yardstick) 

library(recipes)

periods_train <- 60 * 24 * 1.5 
periods_test <- 60 * 24 
skip_span <- 60 * 1 * 10

rolling_origin_resamples <- rolling_origin(pooled_weekday,
                                           initial = periods_train,
                                           assess = periods_test,
                                           cumulative = FALSE,
                                           skip = skip_span
                                        
)

rolling_origin_resamples
plot_split <- function(split,
                       expand_y_axis = TRUE,
                       alpha = 1,
                       size = 1,
                       base_size = 14,
                       main_data = NULL){
  train_tbl <- training(split) %>% 
    add_column(key = "training")
  
  test_tbl <- testing(split) %>% 
    add_column(key = "testing")
  
  data_manipulated <- bind_rows(train_tbl,
                                test_tbl) %>% 
    as_tbl_time(index = index) %>% 
    mutate(key = fct_relevel(key,
                             "training",
                             "testing"))
  
  train_time_summary <- train_tbl %>% 
    tk_index() %>% 
    tk_get_timeseries_summary()
  
    test_time_summary <- test_tbl %>% 
    tk_index() %>% 
    tk_get_timeseries_summary()

  g <- data_manipulated %>% 
    ggplot(aes(x = index, y = y,
               color = key)) +
        geom_line(size = size,
              alpha = alpha) +
    theme_tq(base_size) +
    scale_color_tq() +
    labs(title = glue("Split: {split$id}"),
         subtitle = glue("{train_time_summary$start} to ",
                         "{test_time_summary$end}"),
         y = "",
         x = "") +
    theme(legend.position = "none")
  
   

  
  if (expand_y_axis == TRUE){
    time_summary <- main_data %>% 
      tk_index() %>% 
      tk_get_timeseries_summary()
    
    g <- g +
      scale_x_datetime(limits = c(time_summary$start,
                              time_summary$end))
  }
  
  g
}

rolling_origin_resamples$splits[[3]] %>% 
  plot_split(expand_y_axis = TRUE, main_data = pooled_weekday) +
  theme(legend.position = "bottom")

```

Now plotting the entire sampling plan

```{r sampling_plan_plots}
plot_sampling_plan <- function(sampling_tbl,
                               expand_y_axis = TRUE,
                               main_data = NULL,
                               ncol = 3,
                               alpha = 1,
                               size = 1,
                               base_size = 14,
                               title = "Sampling Plan"){
  
  sampling_tbl_with_plots <- sampling_tbl %>% 
    mutate(gg_plots = map(splits,
                          plot_split,
                          expand_y_axis = expand_y_axis,
                          main_data = main_data,
                          alpha = alpha,
                          base_size = base_size))
  
  plot_list <- sampling_tbl_with_plots$gg_plots
  
  p_temp <- plot_list[[1]] + theme(legend.position = "bottom")
  legend <- get_legend(p_temp)
  
  p_body <- plot_grid(plotlist = plot_list,
                      ncol = ncol)
  p_title <- ggdraw() +
    draw_label(title,
               size = 14,
               fontface =  "bold",
               colour = palette_light()[[1]])
  g <- plot_grid(p_title,
                 p_body,
                 legend,
                 ncol = 1,
                 rel_heights =  c(0.05, 1, 0.05))
  g
}

rolling_origin_resamples %>% 
  plot_sampling_plan(main_data = pooled_weekday,
                     ncol = 3)
```

## Creating LTSM Model

Using example split
```{r}
example_split    <- rolling_origin_resamples$splits[[6]]
example_split_id <- rolling_origin_resamples$id[[6]]

df_trn <- analysis(example_split)[1:800, , drop = FALSE]
df_val <- analysis(example_split)[801:1200, , drop = FALSE]
df_tst <- assessment(example_split)


df <- bind_rows(df_trn %>% add_column(key = "training"),
                df_val %>% add_column(key = "validation"),
                df_tst %>% add_column(key = "testing")) %>% 
  as_tbl_time(index = index)
df %>%
  head() %>% 
  kable()

```

using recipes library
```{r}
rec_obj <- recipe(y ~ ., df) %>% 
  step_sqrt(y) %>% 
  step_center(y) %>% 
  step_scale(y) %>% 
  prep()
df_processed_tbl <- bake(rec_obj,
                           df)

center_history <- rec_obj$steps[[2]]$means["y"]
scale_history <- rec_obj$steps[[3]]$sds["y"]

```


transforming data
```{r}

n_timesteps <- 12
n_predictions <- n_timesteps
batch_size <- 10

batch_size <-  10
build_matrix <- function(tseries, overall_timesteps) {
  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) 
    tseries[x:(x + overall_timesteps - 1)]))
}

reshape_X_3d <- function(X){
  dim(X) <- c(dim(X)[1], dim(X)[2], 1)
  X
}

train_vals <- df_processed_tbl %>% 
  filter(key == "training") %>% 
  select(y) %>% 
  pull()
valid_vals <- df_processed_tbl %>% 
  filter(key == "validation") %>% 
  select(y) %>% 
  pull()
test_vals <- df_processed_tbl %>% 
  filter(key == "testing") %>% 
  select(y) %>%
  pull()

# build the windowed matrices
train_matrix <-
  build_matrix(train_vals, n_timesteps + n_predictions)
valid_matrix <-
  build_matrix(valid_vals, n_timesteps + n_predictions)
test_matrix <- build_matrix(test_vals, n_timesteps + n_predictions)

# separate matrices into training and testing parts
# also, discard last batch if there are fewer than batch_size samples
# (a purely technical requirement)
X_train <- train_matrix[, 1:n_timesteps]
y_train <- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_train <- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]
y_train <- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]

X_valid <- valid_matrix[, 1:n_timesteps]
y_valid <- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_valid <- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]
y_valid <- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]

X_test <- test_matrix[, 1:n_timesteps]
y_test <- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_test <- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]
y_test <- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]
# add on the required third axis
X_train <- reshape_X_3d(X_train)
X_valid <- reshape_X_3d(X_valid)
X_test <- reshape_X_3d(X_test)

y_train <- reshape_X_3d(y_train)
y_valid <- reshape_X_3d(y_valid)
y_test <- reshape_X_3d(y_test)

```


```{r}

library(keras)
library(tfruns)

FLAGS <- flags(
  # There is a so-called "stateful LSTM" in Keras. While LSTM is stateful
  # per se, this adds a further tweak where the hidden states get 
  # initialized with values from the item at same position in the previous
  # batch. This is helpful just under specific circumstances, or if you want
  # to create an "infinite stream" of states, in which case you'd use 1 as 
  # the batch size. Below, we show how the code would have to be changed to
  # use this, but it won't be further discussed here.
  flag_boolean("stateful", FALSE),
  # Should we use several layers of LSTM?
  # Again, just included for completeness, it did not yield any superior 
  # performance on this task.
  # This will actually stack exactly one additional layer of LSTM units.
  flag_boolean("stack_layers", FALSE),
  # number of samples fed to the model in one go
  flag_integer("batch_size", 10),
  # size of the hidden state, equals size of predictions
  flag_integer("n_timesteps", 12),
  # how many epochs to train for
  flag_integer("n_epochs", 100),
  # fraction of the units to drop for the linear transformation of the inputs
  flag_numeric("dropout", 0.2),
  # fraction of the units to drop for the linear transformation of the 
  # recurrent state
  flag_numeric("recurrent_dropout", 0.2),
  # loss function. Found to work better for this specific case than mean
  # squared error
  flag_string("loss", "logcosh"),
  # optimizer = stochastic gradient descent. Seemed to work better than adam 
  # or rmsprop here (as indicated by limited testing)
  flag_string("optimizer_type", "sgd"),
  # size of the LSTM layer
  flag_integer("n_units", 128),
  # learning rate
  flag_numeric("lr", 0.003),
  # momentum, an additional parameter to the SGD optimizer
  flag_numeric("momentum", 0.9),
  # parameter to the early stopping callback
  flag_integer("patience", 10)
)

# the number of predictions we'll make equals the length of the hidden state
n_predictions <- FLAGS$n_timesteps
# how many features = predictors we have
n_features <- 1
# just in case we wanted to try different optimizers, we could add here
optimizer <- switch(FLAGS$optimizer_type,
                    sgd = optimizer_sgd(lr = FLAGS$lr, 
                                        momentum = FLAGS$momentum)
                    )

# callbacks to be passed to the fit() function
# We just use one here: we may stop before n_epochs if the loss on the
# validation set does not decrease (by a configurable amount, over a 
# configurable time)
callbacks <- list(
  callback_early_stopping(patience = FLAGS$patience)
)
```

```{r}
# create the model
model <- keras_model_sequential()

# add layers
# we have just two, the LSTM and the time_distributed 
model %>%
  layer_lstm(
    units = FLAGS$n_units, 
    # the first layer in a model needs to know the shape of the input data
    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
    dropout = FLAGS$dropout,
    recurrent_dropout = FLAGS$recurrent_dropout,
    # by default, an LSTM just returns the final state
    return_sequences = TRUE
  ) %>% time_distributed(layer_dense(units = 1))

model %>%
  compile(
    loss = FLAGS$loss,
    optimizer = optimizer,
    # in addition to the loss, Keras will inform us about current 
    # MSE while training
    metrics = list("mean_squared_error")
  )

history <- model %>% fit(
  x          = X_train,
  y          = y_train,
  validation_data = list(X_valid, y_valid),
  batch_size = FLAGS$batch_size,
  epochs     = FLAGS$n_epochs,
  callbacks = callbacks
)

```

```{r}
plot(history, metrics = "loss")


pred_train <- model %>%
  predict(X_train, batch_size = FLAGS$batch_size) %>%
  .[, , 1]
pred_train



# Retransform values to original scale
pred_train <- (pred_train * scale_history + center_history) ^2
compare_train <- df %>% filter(key == "training")

# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_train)) {
  varname <- paste0("pred_train", i)
  compare_train <-
    mutate(compare_train,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_train[i,],
      rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}

```


```{r}
coln <- colnames(compare_train)[4:ncol(compare_train)]
cols <- map(coln, quo(sym(.)))
rsme_train <-
  map_dbl(cols, function(col)
    rmse(
      compare_train,
      truth = y,
      estimate = !!col,
      na.rm = TRUE
    )) %>% mean()

rsme_train
```


```{r}
p <- ggplot(compare_train, aes(x = index, y = y)) + geom_line(alpha = 0.2) +
  geom_line(size = 1,aes(y = pred_train1), color = "cyan") +
  geom_line(size = 1,aes(y = pred_train50), color = "red") +
  geom_line(size = 1,aes(y = pred_train100), color = "green") +
  geom_line(size = 1,aes(y = pred_train150), color = "violet") +
  geom_line(size = 1,aes(y = pred_train200), color = "cyan") +
  geom_line(size = 1,aes(y = pred_train250), color = "red") +
  geom_line(size = 1,aes(y = pred_train300), color = "red") +
  geom_line(size = 1,aes(y = pred_train350), color = "green") +
  geom_line(size = 1,aes(y = pred_train400), color = "cyan") +
  geom_line(size = 1,aes(y = pred_train450), color = "red") +
  geom_line(size = 1,aes(y = pred_train500), color = "green") +
  geom_line(size = 1,aes(y = pred_train550), color = "violet") +
  geom_line(size = 1,aes(y = pred_train600), color = "cyan") +
  geom_line(size = 1,aes(y = pred_train650), color = "red") +
  geom_line(size = 1,aes(y = pred_train700), color = "red") +
  geom_line(size = 1,aes(y = pred_train750), color = "green") +
  ggtitle("Predictions on the training set") +
  theme_minimal()
ggplotly(p)
```



```{r}
pred_test <- model %>%
  predict(X_test, batch_size = FLAGS$batch_size) %>%
  .[, , 1]

# Retransform values to original scale
pred_test <- (pred_test * scale_history + center_history) ^2
pred_test[1:10, 1:5] %>% print()
compare_test <- df %>% filter(key == "testing")

# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_test)) {
  varname <- paste0("pred_test", i)
  compare_test <-
    mutate(compare_test,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_test[i,],
      rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}

compare_test %>% write_csv(str_replace(model_path, ".hdf5", ".test.csv"))
compare_test[FLAGS$n_timesteps:(FLAGS$n_timesteps + 10), c(2, 4:8)] %>% print()

coln <- colnames(compare_test)[4:ncol(compare_test)]
cols <- map(coln, quo(sym(.)))
rsme_test <-
  map_dbl(cols, function(col)
    rmse(
      compare_test,
      truth = y,
      estimate = !!col,
      na.rm = TRUE
    )) %>% mean()

rsme_test
```



```{r}
p <- ggplot(compare_test, aes(x = index, y = y)) + geom_line(alpha = 0.2) +
  geom_line(aes(y = pred_test1), color = "cyan") +
  geom_line(aes(y = pred_test50), color = "red") +
  geom_line(aes(y = pred_test100), color = "green") +
  geom_line(aes(y = pred_test150), color = "violet") +
  geom_line(aes(y = pred_test200), color = "cyan") +
  geom_line(aes(y = pred_test250), color = "red") +
  geom_line(aes(y = pred_test300), color = "green") +
  geom_line(aes(y = pred_test350), color = "cyan") +
  geom_line(aes(y = pred_test400), color = "red") +
  geom_line(aes(y = pred_test450), color = "green") +  
  geom_line(aes(y = pred_test500), color = "cyan") +
  geom_line(aes(y = pred_test550), color = "violet") +
  ggtitle("Predictions on test set") +
  theme_minimal()
ggplotly(p)
```


All splits

```{r}
obtain_predictions <- function(split) {
  df_trn <- analysis(split)[1:800, , drop = FALSE]
  df_val <- analysis(split)[801:1200, , drop = FALSE]
  df_tst <- assessment(split)
  
  df <- bind_rows(
    df_trn %>% add_column(key = "training"),
    df_val %>% add_column(key = "validation"),
    df_tst %>% add_column(key = "testing")
  ) %>%
    as_tbl_time(index = index)
  
  rec_obj <- recipe(y ~ ., df) %>%
    step_sqrt(y) %>%
    step_center(y) %>%
    step_scale(y) %>%
    prep()
  
  df_processed_tbl <- bake(rec_obj, df)
  
  center_history <- rec_obj$steps[[2]]$means["y"]
  scale_history  <- rec_obj$steps[[3]]$sds["y"]
  
  FLAGS <- flags(
    flag_boolean("stateful", FALSE),
    flag_boolean("stack_layers", FALSE),
    flag_integer("batch_size", 10),
    flag_integer("n_timesteps", 12),
    flag_integer("n_epochs", 100),
    flag_numeric("dropout", 0.2),
    flag_numeric("recurrent_dropout", 0.2),
    flag_string("loss", "logcosh"),
    flag_string("optimizer_type", "sgd"),
    flag_integer("n_units", 128),
    flag_numeric("lr", 0.003),
    flag_numeric("momentum", 0.9),
    flag_integer("patience", 10)
  )
  
  n_predictions <- FLAGS$n_timesteps
  n_features <- 1
  
  optimizer <- switch(FLAGS$optimizer_type,
                      sgd = optimizer_sgd(lr = FLAGS$lr, momentum = FLAGS$momentum))
  callbacks <- list(
    callback_early_stopping(patience = FLAGS$patience)
  )
  
  train_vals <- df_processed_tbl %>%
    filter(key == "training") %>%
    select(y) %>%
    pull()
  valid_vals <- df_processed_tbl %>%
    filter(key == "validation") %>%
    select(y) %>%
    pull()
  test_vals <- df_processed_tbl %>%
    filter(key == "testing") %>%
    select(y) %>%
    pull()
  
  train_matrix <-
    build_matrix(train_vals, FLAGS$n_timesteps + n_predictions)
  valid_matrix <-
    build_matrix(valid_vals, FLAGS$n_timesteps + n_predictions)
  test_matrix <-
    build_matrix(test_vals, FLAGS$n_timesteps + n_predictions)
  
  X_train <- train_matrix[, 1:FLAGS$n_timesteps]
  y_train <-
    train_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_train <-
    X_train[1:(nrow(X_train) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_train <-
    y_train[1:(nrow(y_train) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_valid <- valid_matrix[, 1:FLAGS$n_timesteps]
  y_valid <-
    valid_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_valid <-
    X_valid[1:(nrow(X_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_valid <-
    y_valid[1:(nrow(y_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_test <- test_matrix[, 1:FLAGS$n_timesteps]
  y_test <-
    test_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_test <-
    X_test[1:(nrow(X_test) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_test <-
    y_test[1:(nrow(y_test) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_train <- reshape_X_3d(X_train)
  X_valid <- reshape_X_3d(X_valid)
  X_test <- reshape_X_3d(X_test)
  
  y_train <- reshape_X_3d(y_train)
  y_valid <- reshape_X_3d(y_valid)
  y_test <- reshape_X_3d(y_test)
  
  model <- keras_model_sequential()
  
  model %>%
    layer_lstm(
      units            = FLAGS$n_units,
      batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
      dropout = FLAGS$dropout,
      recurrent_dropout = FLAGS$recurrent_dropout,
      return_sequences = TRUE
    )     %>% time_distributed(layer_dense(units = 1))
  
  model %>%
    compile(
      loss = FLAGS$loss,
      optimizer = optimizer,
      metrics = list("mean_squared_error")
    )
  
  model %>% fit(
    x          = X_train,
    y          = y_train,
    validation_data = list(X_valid, y_valid),
    batch_size = FLAGS$batch_size,
    epochs     = FLAGS$n_epochs,
    callbacks = callbacks
  )
  
  
  pred_train <- model %>%
    predict(X_train, batch_size = FLAGS$batch_size) %>%
    .[, , 1]
  
  # Retransform values
  pred_train <- (pred_train * scale_history + center_history) ^ 2
  compare_train <- df %>% filter(key == "training")
  
  for (i in 1:nrow(pred_train)) {
    varname <- paste0("pred_train", i)
    compare_train <-
      mutate(compare_train, !!varname := c(
        rep(NA, FLAGS$n_timesteps + i - 1),
        pred_train[i, ],
        rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
      ))
  }
  
  pred_test <- model %>%
    predict(X_test, batch_size = FLAGS$batch_size) %>%
    .[, , 1]
  
  # Retransform ys
  pred_test <- (pred_test * scale_history + center_history) ^ 2
  compare_test <- df %>% filter(key == "testing")
  
  for (i in 1:nrow(pred_test)) {
    varname <- paste0("pred_test", i)
    compare_test <-
      mutate(compare_test, !!varname := c(
        rep(NA, FLAGS$n_timesteps + i - 1),
        pred_test[i, ],
        rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)
      ))
  }
  list(train = compare_train, test = compare_test)
  
}

all_split_preds <- rolling_origin_resamples %>%
     mutate(predict = map(splits, obtain_predictions))
```



```{r}
calc_rmse <- function(df) {
  coln <- colnames(df)[4:ncol(df)]
  cols <- map(coln, quo(sym(.)))
  map_dbl(cols, function(col)
    rmse(
      df,
      truth = y,
      estimate = !!col,
      na.rm = TRUE
    )) %>% mean()
}

all_split_preds <- all_split_preds %>% unnest(predict)
all_split_preds_train <- all_split_preds[seq(1, 11, by = 2), ]
all_split_preds_test <- all_split_preds[seq(2, 12, by = 2), ]

all_split_rmses_train <- all_split_preds_train %>%
  mutate(rmse = map_dbl(predict, calc_rmse)) %>%
  select(id, rmse)

all_split_rmses_test <- all_split_preds_test %>%
  mutate(rmse = map_dbl(predict, calc_rmse)) %>%
  select(id, rmse)

all_split_rmses_train

all_split_rmses_train
```


```{r}
plot_train <- function(slice, name) {
  p <- ggplot(slice, aes(x = index, y = y)) + geom_line(alpha = 0.2) +
    geom_line(aes(y = pred_train1), color = "cyan") +
    geom_line(aes(y = pred_train50), color = "red") +
    geom_line(aes(y = pred_train100), color = "green") +
    geom_line(aes(y = pred_train150), color = "violet") +
    geom_line(aes(y = pred_train200), color = "cyan") +
    geom_line(aes(y = pred_train250), color = "red") +
    geom_line(aes(y = pred_train300), color = "red") +
    geom_line(aes(y = pred_train350), color = "green") +
    geom_line(aes(y = pred_train400), color = "cyan") +
    geom_line(aes(y = pred_train450), color = "red") +
    geom_line(aes(y = pred_train500), color = "green") +
    geom_line(aes(y = pred_train550), color = "violet") +
    geom_line(aes(y = pred_train600), color = "cyan") +
    geom_line(aes(y = pred_train650), color = "red") +
    geom_line(aes(y = pred_train700), color = "red") +
    geom_line(aes(y = pred_train750), color = "green") +
    ggtitle(name)
}

train_plots <- map2(all_split_preds_train$predict, all_split_preds_train$id, plot_train)
p_body_train  <- plot_grid(plotlist = train_plots, ncol = 3)
p_title_train <- ggdraw() + 
  draw_label("Backtested Predictions: Training Sets", size = 18, fontface = "bold")

plot_grid(p_title_train, p_body_train, ncol = 1, rel_heights = c(0.05, 1, 0.05))
```


```{r}
plot_test <- function(slice, name) {
  ggplot(slice, aes(x = index, y = y)) + geom_line(alpha = 0.2) +
    geom_line(aes(y = pred_test1), color = "cyan") +
    geom_line(aes(y = pred_test50), color = "red") +
    geom_line(aes(y = pred_test100), color = "green") +
    geom_line(aes(y = pred_test150), color = "violet") +
    geom_line(aes(y = pred_test200), color = "cyan") +
    geom_line(aes(y = pred_test250), color = "red") +
    geom_line(aes(y = pred_test300), color = "green") +
    geom_line(aes(y = pred_test350), color = "cyan") +
    geom_line(aes(y = pred_test400), color = "red") +
    geom_line(aes(y = pred_test450), color = "green") +  
    geom_line(aes(y = pred_test500), color = "cyan") +
    geom_line(aes(y = pred_test550), color = "violet") +
    ggtitle(name)
}

test_plots <- map2(all_split_preds_test$predict, all_split_preds_test$id, plot_test)

p_body_test  <- plot_grid(plotlist = test_plots, ncol = 3)
p_title_test <- ggdraw() + 
  draw_label("Backtested Predictions: Test Sets", size = 18, fontface = "bold")

plot_grid(p_title_test, p_body_test, ncol = 1, rel_heights = c(0.05, 1, 0.05))

```

## Modelling Completely Pooled

```{r, eval=FALSE}
library(tibbletime)
library(dynlm)


pooled_data_original <- oyster_underground_lubridate %>% 
    filter(daytype != "Sat" & daytype != "Sun") %>% 
  group_by(EntTimeHHMM, EntTime, daytype) %>% 
  summarise(y = n()) %>% 
  as_tbl_time(index = EntTimeHHMM)

pooled_data_manipulated <- pooled_data_original %>% 
  group_by(daytype) %>% 
  mutate(y_diff = y - lag(y))

pooled_monday <- pooled_data_manipulated %>% 
  filter(daytype == "Mon")

pooled_train <- pooled_data_manipulated %>% 
  filter(daytype != "Mon")

ggplot(pooled_train %>% 
         filter(daytype == "Tue"),
       aes(x = EntTimeHHMM,
           y = y_diff)) +
  geom_point()

model_0 <- glm(y ~ EntTime,
              data = pooled_train,
              family = poisson)
summary(model_0)

preds <- predict(model_0,
                newdata = pooled_monday %>% 
                  select(EntTime))
pooled_monday$predictions <- preds

ggplot(pooled_monday,
       aes(x = EntTimeHHMM,
           y = y)) +
  geom_point() +
  geom_point(aes(x = EntTimeHHMM,
                 y = predictions), colour = 'red')



model_1 <- lm(y_diff ~ EntTimeHHMM,
               data = pooled_train)

summary(model_1)

## Creating lags
# pooled_data <- pooled_data %>%  
#   group_by(StartStn) %>% 
#   mutate(y_d = y - lag(y)) %>% 
#   ungroup()

simple_ar_1 <- dynlm(d(y) ~ L(y, 1),
                      data = pooled_data %>% 
                       select(y) %>% 
                       as.zoo())
summary(simple_ar_1)


p_1 <- ggplot(pooled_data,
       aes(x = EntTimeHHMM,
           y = y_d)) +
  geom_point()

p_1

p_2 <- ggplot(pooled_data,
              aes(x = EntTimeHHMM,
                  y = y)) +
  geom_line()
p_2

subplot(p_1, p_2)


p_3 <- ggplot(pooled_data,
              aes(x = EntTimeHHMM,
                  y = y_d*10)) +
  geom_point(alpha = 0.2) +
  geom_line(aes(x = EntTimeHHMM,
                y = y))
p_3



```


```{r eval = FALSE}

## TODO: time series stuff AR process, exponential CS fit, pool station data. LTSM. XGBoost

## plot y histogram





stan_data <- oyster_underground_lubridate %>%
  filter(StartStn == "Waterloo JLE") %>% 
  filter(daytype != "Sat" | daytype != "Sun") %>% 
  group_by(EntTimeHHMM, EntTime) %>% 
  summarise(y = n()) %>% 
  mutate(post_lunch = ifelse(test = (EntTimeHHMM >= hms("12:00:00")),
                             1, 2))

stan_data

ggplot(stan_data,
       aes(y)) +
  geom_histogram(bins = 60)



stan_data %>% 
  ggplot(aes(x = EntTimeHHMM, y = y)) +
  geom_point() +
  theme_minimal()


ed <- lm(data = stan_data,formula =  y ~ EntTime)
ed %>% summary()

library(rstanarm)
options(mc.cores = parallel::detectCores())
ed_two <- stan_glmer(data = stan_data,
                   y ~ 1 | post_lunch,
                   family = poisson,
                   control = list(adapt_delta = 0.99))

ed_two %>% launch_shinystan()

library(rstan)
library(shinystan)

stan_data_list <- list(y = stan_data$y,
               N = nrow(stan_data),
               id = stan_data$post_lunch,
               L = 2)

model <- stan('_posts/waterloo_footfall_hierarchical_2.stan',
              data = stan_data_list,
              control = list(adapt_delta = 0.999))

model %>% launch_shinystan()


  

```
