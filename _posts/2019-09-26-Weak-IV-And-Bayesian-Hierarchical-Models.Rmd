---
output: 
  html_document:
    code_folding: show
    highlight: kate
    theme: paper
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      cache = TRUE,
                      fig.align = "centre")
library(mvtnorm)
library(AER)
library(tidyverse)
library(broom)
library(rstan)
library(tidybayes)
library(ggstance)
```


In this post we're going to demonstrate the use of Bayesian hierarchical models to overcome the weak instruments problem. We essentially recreate the work of [Chamberlain and Imbens, 2003](https://doi.org/10.1111/j.1468-0262.2004.00485.x) which I definitely should have read _before_ embarking on this project and not _after_ - ah well, we live and learn.



# Setting up The Problem

There's a lot of material on weak instruments online so we'll keep this short - a simple, canonical weak instrument problem looks something like this:


$$
\begin{aligned}
\beta - \hat{\beta} &= (Z'X)^{-1}Z'\varepsilon \\
&\rightarrow (0)^{-1}(0)
\end{aligned}
$$

Even asympotically as $N$ grows the first stage relationship effectively disappears. We can think of an instrument as an exogenous shock to one part of the system (thank you exlcusion restriction) and in the weak instruments problem this shock is too small - we get a nasty limiting distribution. 




Rephrasing the above along Mostly Harmless lines using the wald estimator we have:

$$
\begin{aligned}
y_i &= z_i\gamma + u_i \text{  (Reduced Form)}\\
x_i &= z_i\pi + e_i \text{ (First Stage)} \\ \\
\hat{\beta}_{iv} &= \frac{\hat{\gamma}}{\hat{\pi}}
\end{aligned}
$$

How do we fix this? Well a good way would be to stop dividing by 0 - bad things tend to happen when we divide by 0.


# A Simple Bayesian Approach


Let's embed the above in a Bayesian setting, focusing on the first stage for now:
$$
\begin{aligned}
\pi &\sim N(\mu, \lambda^{-1} ) \\
X &= Z\pi + e \\ 
\pi_{\text{posterior}} &= (Z'Z + \lambda)^{-1}(Z'Z\hat{\pi} + \lambda\mu)
\end{aligned}
$$


The first line indicates lays out our prior over $\pi$, we've used a precision parametrisation to make the interpretation of $\lambda$ easier. The second line just swtiches notation slightly towards a matrix algebra formulation. Finally, we come to the posterior. 

The posterior of $\pi$ is very similar to the frequentist solution but we have this extra $\lambda$ term - the interpretation here is that the posterior is essentially a precision weighted average of our prior and the data where $\hat{\pi}$ is the least squares solution familiar to frequentists. 

# How does this help us?

Well, we require $\pi_{\text{posterior}} \nrightarrow 0$ whilst $\hat{\pi} \rightarrow 0$. Therefore, we _really, really_ want $\mu \nrightarrow 0$.


This gives us two potential solutions:

- Non-centred prior, $\pi \sim N(f(n)\times\mu, \lambda^{-1})$ where $\mu \neq 0$ (Don't do this, please).

- __Bayesian Hierarchical model, $\hat{\mu}$ is a precision weighted average of $\hat{\pi}_j$ for all groups/levels across our data.__ 




We're going to focus on the second bullet point in the rest of this post.

# Bayesian Hierarchical Model


Suppose we observe multiple natural groups or levels in our data indexed by $j$ - maybe a treatment effect from a federal policy we observe across states or cities. In some states we might face a very weak instrument but in others a much stronger instrument. We know that what we learn from one treatment effect should probably generalise, to a degree, to another city. A Bayesian hierarchical model lets us put structure over this generalisability; we want to separate sampling variation from true heterogeneity in treatment effects; introduce regularisation through hierarchical shrinkage and overcome the weak instruments problem - a BHM brings all these good things to the table.



Sometimes a BHM is simplest to think about when we draw it, here $\beta_j$ indexes _cities_ __not__ _regressors_:


<!-- ![](../plots/DIA_BHM.png) -->




But we're not quite done - we want to perform instrumental variables estimation, not simple OLS. The solution - add another dimension:

$$
\begin{aligned}
\mathcal{Y}_{j} &= (\textbf{Y}_{j} \quad \textbf{X}_j) \\
\mathcal{Y}_j &\sim MVN( \mathcal{X}_j \mathcal{B}_j, \Omega_j) \quad \forall i, j
\end{aligned}
$$

where:

$$
\begin{aligned}
\text{Y}_{ij} &= \widehat{\text{X}}^n_{ij}\beta_j^{iv} + X_{ij}^e\beta^{e}_j + u_{ij} \\  
\text{X}_{ij} &= \text{Z}_{ij}\pi_j + X^e_{ij}\gamma_j + e_{ij}
\end{aligned}
$$

Stacking into block matrices gives:
$$
\begin{aligned}
(\textbf{Y}_j \quad \textbf{X}_{j}) &= (\text{Z}_{ij} \quad X^e_{ij}) \begin{pmatrix}
\pi_j \beta^{iv}_{j} & \pi_j \\
\gamma_j \beta^{iv}_j + \beta^e_j & \gamma_j
\end{pmatrix} \\
&= \mathcal{X}_j (\mathbf{b}^r \quad \mathbf{b}^f) \\
 &= \mathcal{X}_j \mathcal{B}_j
 \end{aligned}
$$

We need to use the multivariate normal here to fully account for posterior uncertainty across both relationships. Modelling the problem as two independent normals is the Bayesian analogue to computing two-stage least squares manually and forgetting to adjust our standard errors.


Where the multivariate normal covariance matrix describes the first stage and reduced form's variance and covariance:


$$
\Omega_j = \begin{pmatrix}
\sigma^{2}_{r,j} & \sigma_{fr, j} \\
\sigma_{fr, j} & \sigma^{2}_{f, j}
\end{pmatrix}
$$


## Priors


We need to put some priors on all of the above, here we use a pretty standard set up:




$$
\begin{aligned}
    \mathbf{b}^{d}_j &\sim MVN(\mu^{d}, \Sigma^d) & r, f \in d \\
    \mu^{r}, \mu^{f} &\sim N(0, 10)
\end{aligned}
$$


As is common in Bayesian estimation we split our covariance matrices up into a scale and correlation matrix:

$$
\begin{aligned}
\Sigma &= \text{diag}(\tau) \Phi \text{diag}(\tau) \\
\Omega_j &= \text{diag}(\nu_j) \Theta_j \text{diag}(\nu_j)
\end{aligned}
$$




with priors:

$$
\begin{aligned}
\tau, \nu &\sim \text{Cauchy}^+(0, 5) \\
\Phi, \Theta_j &\sim \text{LKJ}(2)
\end{aligned}
$$



That's almost all greek alphabet I know, so fortunately we can turn to actually estimating this monster now.



## Sufficient Statistics


Unfortunately, fitting this model in stan would take a __long__ time. Hierarchical models are notoriously hard to fit and we're bumping the difficulty up somewhat by estimating a multivariate hierarchical model.


Fortunately, help is at hand - we're using a multivariate normal likelihood which means we can exploit the fact that the normal is in the exponential family of distributions and therefore the factorisation criteria means we can find sufficient statistics for the normal likelihood. For the univariate case, suppose we're estimating the mean of $n$ $x_i$ draws from a gaussian distribution, rather than evaluating the likelihood conditioning on _all_ the data we can just use $\sum^n_i{x_i}$ and $\sum^n_i{x_i^2}$ to evaluate the likelihood.


This is an incredible speed up for stan - rather than looping over $N$ datapoints we can loop over $J$ sets of sufficient statistics.


To move from the univariate to multivariate case we're going to reparametrise the problem to create a residual matrix and compute $\bar{\mathbf{x}}$ and $\hat{\Sigma}$


Therefore, define a residual matrix:
$$
R = (Y_{j} \quad X^n_j)  -  (Z_{j} \quad X^e_{j}) \begin{pmatrix}
\pi_j \beta^{iv}_{j} & \pi_j \\
\gamma_j \beta^{iv}_j + \beta^e_j & \gamma_j
\end{pmatrix} \\
$$

For simplicities sake let's rewrite this as:

$$
R = (a - db)
$$
Therefore we need to factor the likelihood:


$$
L = (2\pi)^{\frac{-n}{2}}|\Sigma|^{-\frac{n}{2}}exp(-\frac{1}{2} R'\Sigma^{-1}R)
$$

Into something like this:
$$
\text{log} L \approx -\frac{n}{2}\Sigma^{-1}\bar{R'}\bar{R} - \frac{1}{2}  \text{trace}(\Sigma^{-1}R'R)
$$

I'm not sure this is the most elegant formulation but it was the easiest for me to debug in R.

Calculating $R'R$ (and the analogous  $\bar{R'}\bar{R}$) is just a matter of expanding out $(a' - b'd')(a-db)$:

$$
\begin{aligned}
R'R &= a'a - 2a'db + b'd'db \\
a'a &= \pmatrix{Y'Y \quad Y'X_n \\
                X'_nY \quad X'_nX_n} \\
a'd &= \pmatrix{Y'Z \quad Y'X_e \\
                  X'_nZ \quad X'_nX_e} \\
                  
d'd &= \pmatrix{Z'Z \quad Z'X_e \\
                X'_eZ \quad X'_eX_e}
\end{aligned}
$$


That's the last of the algebra - I promise. It's worth noting that the elements of each matrix are themselves block matrices, writing the sufficient statistics out in their most general format makes it easier to add multiple instruments and covariates later down the line.

# Generating Fake Data



First we're going to define a function to generate our model coefficients. Here we've modelled our coefficients of interest, $\beta^{iv}_j$, as normal with mean 1 and variance 1. The first stage relationship of interest, $\pi_j$, will be centred at -2 with variance 1 also - however we've set $\pi_1 = 0$ for every draw of the simulation. Our exogenous variables are centred at 0, again with variance 1. 


```{r}

create_betas <- function(J = 5){
  mu_iv <- 1
  mu_fs <- -2

  beta_iv <- rnorm(J, mu_iv, 1)
  beta_fs <- rnorm(J, mu_fs, 1)
  beta_exog <- rnorm(J, 0, 1)
  beta_fs[1] <- 0
  beta_fs
  return(list(
    beta_iv = beta_iv,
    beta_exog = beta_exog,
    beta_fs = beta_fs
  ))
}
```




<!-- ```{r} -->
<!-- source("code/SS_functions.R") -->
<!-- rstan_options(auto_write = TRUE) -->
<!-- options(mc.cores = 4) -->

<!-- ``` -->


Now we're going to generate our $X, Z \text{ and } Y$ data:
```{r}
create_hier_iv_sim_data <- function(n = 1000,
                                    J = 5,
                                    beta_iv,
                                    beta_fs,
                                    beta_exog){


  z <- rnorm(n)
  confounder <- rnorm(n)
  x_exog <- rnorm(n)

  Sigma = matrix(c(5,2.5,2.5,5),2,2)
  ue = rmvnorm(n, rep(0,2), Sigma)

  x_endog <- ue[, 1] + confounder + beta_fs * z
  y <- beta_iv * x_endog +  confounder +  beta_exog * x_exog + ue[, 2]

  df <- data.frame(y,
                   x_endog,
                   z,
                   id = 1:J,
                   x_exog 
                   )

  return(list(data = df,
              beta_iv = beta_iv,
              beta_fs = beta_fs,
              beta_exog = beta_exog))
}

```

In the above function we've created dependence across the errors in each equation by setting the covariance on the off diagonal to 2.5.




Now we create a function to actually fit the frequentist and Bayesian models - `hierarchical_iv_sufficient_statistics` just computes the SSs we derived above. We've set the `adapt_delta` argument to stan's `sampling` extremely high - this model code was originally for a slightly different data generating process and re-parametrising the model just for this post is something I'd rather avoid. One of the advantages of sufficient statistics is that we can be lazy and often bruteforce divergent transitions through a higher adapt delta instead of non-centred parametrisation.  

```{r}
fit_models <- function(sim_data, stan_model = hier_indep_model){
  
  # Calculating no pooling frequentist results using group_map
  freq_result <- sim_data$data %>%
    group_by(id) %>%
    group_map(~ivreg(data = ., y ~ x_endog + x_exog | z + x_exog ) %>% tidy(conf.int = TRUE)) %>%
    map_df(filter, term == "x_endog") %>%
    mutate(true_term = sim_data$beta_iv,
           fist_stage = sim_data$beta_fs,
           beta_exog = sim_data$beta_exog,
           model = "frequentist",
           j = sim_data$data %>% select(id) %>% unique() %>% pull())

  
  # Calculating sufficient statistics to pass to stan
  ss_stan <- hierarchical_iv_sufficient_statistics(sim_data$data,
                                                   "y",
                                                   "x_endog",
                                                   "x_exog",
                                                   "z",
                                                   "id")
  # Sampling from the model
  bhm_results <- sampling(
    stan_model,
    ss_stan,
    chains = 4,
    cores = 4,
    control = list(adapt_delta = 0.9999999999,
                   max_treedepth = 15)
  )

  # Computing posterior medians
  model_draws <-   bhm_results %>%
    gather_draws(beta_iv[j, k]) %>%
    median_qi() %>%
    to_broom_names() %>%
    ungroup() %>%
    mutate(true_term = sim_data$beta_iv,
           model = "BHM")
  # Combining results across models
  both_draws <- bind_rows(
    model_draws,
    freq_result
  )
  # Returning model results and stan object for debugging
  return(list(both_draws = both_draws,
              bhm_model = bhm_results))

}


```



Next up, is the stan model - one of the drawbacks of sufficient statistics is that stan models, which aren't particularly literate at the best of times, are a bit harder to follow:

```{r}
# stan model here

```





Finally, we define an anonymous function to pull all of the above together; record some MCMC diagnostics such as split Rhat and finally, spit out a dataframe with model estimates. The last few lines perform the simulation.

```{r, eval = FALSE}

anon_func <- function(draw){
  betas <- create_betas()
  sim_data <- create_hier_iv_sim_data(n = 10000,
                                      J = 5,
                                      beta_iv = betas$beta_iv,
                                      beta_fs = betas$beta_fs,
                                      beta_exog = betas$beta_exog)
  model_fitted <- fit_models(sim_data,
                              stan_model = hier_indep_model)
  model_results <- model_fitted$both_draws
  n_divergent <- get_sampler_params(model_fitted$bhm_model, inc_warmup = FALSE) %>%
    map(data.frame) %>%
    map_dfr(select, divergent__) %>%
    sum()

  n_rhat_too_high <- (summary(model_fitted$bhm_model))$summary %>%
    as.data.frame() %>%
    filter(Rhat > 1.1) %>%
    nrow()

  problematic_rhat <- (summary(model_fitted$bhm_model))$summary %>%
    as.data.frame() %>%
    filter(Rhat > 1.1) %>%
    rownames_to_column("variable") %>%
    select(variable) %>%
    pull() %>%
    paste0(collapse = ",")

  model_results$divergent <- n_divergent
  model_results$draw <- draw
  model_results$n_rhat_issues <- n_rhat_too_high
  model_results$problematic_rhat_variables <- problematic_rhat
  return(model_results)
}

# Set n core for stan
options(mc.cores = 4)
bhm_simmed_draws <- 1:1000 %>% 
  map_dfr(anon_func)

```


# Results


```{r}
bhm_simmed_draws <- read_csv("../../../../Dropbox-Ed/Ed/DMLD-BHM/data/output_data/model_draws/weak_iv_sim_draws.csv")

```


```{r}
bhm_simmed_draws %>% 
  filter(draw %in% 425 ) %>%
  ggplot(aes(x = estimate,
             xmin = conf.low,
             xmax = conf.high,
             y = j,
             colour = model)) +
  geom_pointrangeh(position = position_dodge2v(0.5)) +
  geom_point(inherit.aes = FALSE,
             aes(x = true_term,
                 y = j), shape = 3
  ) +
  theme_minimal() +
  scale_y_continuous(breaks = 1:18) +
  labs(subtitle = paste0("draw: ", 425),
       title = "Weak IV: BHM vs Frequentist No Pooling")
```







```{r}
bhm_simmed_draws %>% 
  filter(n_rhat_issues == 0) %>%
  filter(divergent <= 10) %>% 
  group_by(model, j) %>% 
  summarise(mse = mean((true_term - estimate)^2)) %>% 
  spread(model, mse) %>% 
  knitr::kable(digits = 4)
```


```{r}


comp_plot_df_width <- bhm_simmed_draws %>% 
  filter(n_rhat_issues == 0) %>%
  filter(divergent < 10) %>% 
  filter(j == 1) %>% 
  mutate(ci_width = abs(conf.high - conf.low),
         ci_width_relative = ci_width / abs(true_term)) %>% 
  group_by(model) %>% 
  mutate(mean_ci_width_rel = log10(mean(ci_width_relative)))

 
comp_plot_shadow <- comp_plot_df_width %>%
  ungroup() %>% 
  select(-model)

comp_plot_df_width %>% 
  ggplot(aes(x = log10(ci_width_relative),
             fill = model)) +
  geom_histogram(data = comp_plot_shadow,
                 fill = "grey") +
  geom_histogram(colour = "black") +
  facet_wrap(~model) +
  theme_minimal() +
  geom_vline(aes(xintercept = mean_ci_width_rel),
             linetype = "longdash") +
  labs(title = "Confidence Interval Width / Point Estimate",
       caption = "log scale, dotted line indicates mean of datapoints.")


```



```{r}
hist_error_sq <- bhm_simmed_draws %>% 
  filter(n_rhat_issues == 0,
         divergent < 10) %>% 
  mutate(error = (true_term - estimate)^2) 

hist_error_sq_subset <- hist_error_sq %>% 
  select(-model)

hist_error_sq %>% 
  ggplot(aes(x = log10(error),
             fill = model)) +
  geom_histogram(data = hist_error_sq_subset,
                 fill = "grey",
                 bins = 45,
                 size = 0.5) +
  geom_histogram(colour = "black",
                 bins = 45,
                 size = 0.5) + 
  facet_grid(j ~ model,
             scales = "free_y") +
  theme_bw()  +
  labs(title = "Squared Error Draws Across Groups",
       subtitle = "Log10 scale")

```


```{r}

library(ggridges)


hist_error_sq %>% 
  ggplot(aes(x = log10(error),
             fill = model,
             y = factor(j))) +
  geom_density_ridges(alpha = 0.7,quantile_lines = TRUE,
                      quantiles = 2
                      ) +
  theme_ridges()
```

